{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFNA7PjIaWsj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pymupdf\n",
        "from glob import glob\n",
        "import json\n",
        "import requests\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE = \"/content/일일 착용 콘택트렌즈의 연속 착용에 따른 세균 오염.pdf\""
      ],
      "metadata": {
        "id": "4mk3RgxvafQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_pdf(filepath, batch_size=2):\n",
        "    \"\"\"\n",
        "    입력 PDF를 여러 개의 작은 PDF 파일로 분할\n",
        "    \"\"\"\n",
        "    # PDF 파일 열기\n",
        "    input_pdf = pymupdf.open(filepath)\n",
        "    num_pages = len(input_pdf)\n",
        "    print(f\"총 페이지 수: {num_pages}\")\n",
        "\n",
        "    ret = []\n",
        "    # PDF 분할\n",
        "    for start_page in range(0, num_pages, batch_size):\n",
        "        end_page = min(start_page + batch_size, num_pages) - 1\n",
        "\n",
        "        # 분할된 PDF 저장\n",
        "        input_file_basename = os.path.splitext(filepath)[0]\n",
        "        output_file = f\"{input_file_basename}_{start_page:04d}_{end_page:04d}.pdf\"\n",
        "        print(f\"분할 PDF 생성: {output_file}\")\n",
        "        with pymupdf.open() as output_pdf:\n",
        "            output_pdf.insert_pdf(input_pdf, from_page=start_page, to_page=end_page)\n",
        "            output_pdf.save(output_file)\n",
        "            ret.append(output_file)\n",
        "\n",
        "    # 입력 PDF 파일 닫기\n",
        "    input_pdf.close()\n",
        "    return ret"
      ],
      "metadata": {
        "id": "ND4Wc38Eag8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_files = split_pdf(SAMPLE)"
      ],
      "metadata": {
        "id": "8ACJZHTIaiPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayoutAnalyzer:\n",
        "  def __init__(self, api_key):\n",
        "    self.api_key = api_key\n",
        "\n",
        "  def _upstage_layout_analysis(self, input_file):\n",
        "    \"\"\"\n",
        "    레이아웃 분석 api 호출\n",
        "    param input_file: 분석할 pdf 파일 경로\n",
        "    param output_file: 분석 결과 저장할 json 파일 경로\n",
        "    \"\"\"\n",
        "    # API 요청 보내기\n",
        "    response = requests.post(\n",
        "        \"https://api.upstage.ai/v1/document-ai/layout-analysis\",\n",
        "        headers = {\"Authorization\": f\"Bearer {self.api_key}\"},\n",
        "        data = {\"ocr\": False},\n",
        "        files = {\"document\": open(input_file, \"rb\")},\n",
        "    )\n",
        "\n",
        "    # 응답 저장\n",
        "    if response.status_code == 200:\n",
        "      output_file = os.path.splitext(input_file)[0] + \".json\"\n",
        "      with open(output_file, \"w\") as f:\n",
        "        json.dump(response.json(), f, ensure_ascii=False)\n",
        "      return output_file\n",
        "    else:\n",
        "      raise ValueError(f\"예상치 못한 상태 코드: {response.status_code}\")\n",
        "\n",
        "  def execute(self, input_file):\n",
        "    return self._upstage_layout_analysis(input_file)"
      ],
      "metadata": {
        "id": "izgP24HoajD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = LayoutAnalyzer(\"up_gs1RLv6GzCaHcCF7eCDgwLD3xDtil\")\n",
        "\n",
        "analyzed_files = []\n",
        "\n",
        "for file in split_files:\n",
        "  analyzed_files.append(analyzer.execute(file))"
      ],
      "metadata": {
        "id": "Tpwa5BBAakNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzed_files"
      ],
      "metadata": {
        "id": "JDoDDD1valWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install markdownify"
      ],
      "metadata": {
        "id": "FhnYG3k5amDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pymupdf\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from markdownify import markdownify as markdown\n",
        "\n",
        "\n",
        "class PDFImageProcessor:\n",
        "    \"\"\"\n",
        "    PDF 이미지 처리를 위한 클래스\n",
        "\n",
        "    PDF 파일에서 이미지를 추출하고, HTML 및 Markdown 형식으로 변환하는 기능을 제공합니다.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pdf_file):\n",
        "        \"\"\"\n",
        "        PDFImageProcessor 클래스의 생성자\n",
        "\n",
        "        :param pdf_file: 처리할 PDF 파일의 경로\n",
        "        \"\"\"\n",
        "        self.pdf_file = pdf_file\n",
        "        self.json_files = sorted(glob(os.path.splitext(pdf_file)[0] + \"*.json\"))\n",
        "        self.output_folder = os.path.splitext(pdf_file)[0]\n",
        "        self.filename = os.path.splitext(os.path.basename(SAMPLE))[0]\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_json(json_file):\n",
        "        \"\"\"\n",
        "        JSON 파일을 로드하는 정적 메서드\n",
        "\n",
        "        :param json_file: 로드할 JSON 파일의 경로\n",
        "        :return: JSON 데이터를 파이썬 객체로 변환한 결과\n",
        "        \"\"\"\n",
        "        with open(json_file, \"r\") as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_page_sizes(json_data):\n",
        "        \"\"\"\n",
        "        각 페이지의 크기 정보를 추출하는 정적 메서드\n",
        "\n",
        "        :param json_data: JSON 데이터\n",
        "        :return: 페이지 번호를 키로, [너비, 높이]를 값으로 하는 딕셔너리\n",
        "        \"\"\"\n",
        "        page_sizes = {}\n",
        "        for page_element in json_data[\"metadata\"][\"pages\"]:\n",
        "            width = page_element[\"width\"]\n",
        "            height = page_element[\"height\"]\n",
        "            page_num = page_element[\"page\"]\n",
        "            page_sizes[page_num] = [width, height]\n",
        "        return page_sizes\n",
        "\n",
        "    def pdf_to_image(self, page_num, dpi=300):\n",
        "        \"\"\"\n",
        "        PDF 파일의 특정 페이지를 이미지로 변환하는 메서드\n",
        "\n",
        "        :param page_num: 변환할 페이지 번호 (1부터 시작)\n",
        "        :param dpi: 이미지 해상도 (기본값: 300)\n",
        "        :return: 변환된 이미지 객체\n",
        "        \"\"\"\n",
        "        with pymupdf.open(self.pdf_file) as doc:\n",
        "            page = doc[page_num - 1].get_pixmap(dpi=dpi)\n",
        "            target_page_size = [page.width, page.height]\n",
        "            page_img = Image.frombytes(\"RGB\", target_page_size, page.samples)\n",
        "        return page_img\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_coordinates(coordinates, output_page_size):\n",
        "        \"\"\"\n",
        "        좌표를 정규화하는 정적 메서드\n",
        "\n",
        "        :param coordinates: 원본 좌표 리스트\n",
        "        :param output_page_size: 출력 페이지 크기 [너비, 높이]\n",
        "        :return: 정규화된 좌표 (x1, y1, x2, y2)\n",
        "        \"\"\"\n",
        "        x_values = [coord[\"x\"] for coord in coordinates]\n",
        "        y_values = [coord[\"y\"] for coord in coordinates]\n",
        "        x1, y1, x2, y2 = min(x_values), min(y_values), max(x_values), max(y_values)\n",
        "\n",
        "        return (\n",
        "            x1 / output_page_size[0],\n",
        "            y1 / output_page_size[1],\n",
        "            x2 / output_page_size[0],\n",
        "            y2 / output_page_size[1],\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def crop_image(img, coordinates, output_file):\n",
        "        \"\"\"\n",
        "        이미지를 주어진 좌표에 따라 자르고 저장하는 정적 메서드\n",
        "\n",
        "        :param img: 원본 이미지 객체\n",
        "        :param coordinates: 정규화된 좌표 (x1, y1, x2, y2)\n",
        "        :param output_file: 저장할 파일 경로\n",
        "        \"\"\"\n",
        "        img_width, img_height = img.size\n",
        "        x1, y1, x2, y2 = [\n",
        "            int(coord * dim)\n",
        "            for coord, dim in zip(coordinates, [img_width, img_height] * 2)\n",
        "        ]\n",
        "        cropped_img = img.crop((x1, y1, x2, y2))\n",
        "        cropped_img.save(output_file)\n",
        "\n",
        "    def extract_images(self):\n",
        "        \"\"\"\n",
        "        전체 이미지 처리 과정을 실행하는 메서드\n",
        "\n",
        "        PDF에서 이미지를 추출하고, HTML 및 Markdown 파일을 생성합니다.\n",
        "        \"\"\"\n",
        "        figure_count = {}  # 페이지별 figure 카운트를 저장하는 딕셔너리\n",
        "\n",
        "        output_folder = self.output_folder\n",
        "        os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "        print(f\"폴더가 생성되었습니다: {output_folder}\")\n",
        "\n",
        "        html_content = []  # HTML 내용을 저장할 리스트\n",
        "\n",
        "        for json_file in self.json_files:\n",
        "            json_data = self._load_json(json_file)\n",
        "            page_sizes = self._get_page_sizes(json_data)\n",
        "\n",
        "            # 파일 이름에서 페이지 범위 추출\n",
        "            page_range = os.path.basename(json_file).split(\"_\")[1:]\n",
        "            start_page = int(page_range[0])\n",
        "\n",
        "            for element in json_data[\"elements\"]:\n",
        "                if element[\"category\"] == \"figure\":\n",
        "                    # 파일 내에서의 상대적인 페이지 번호 계산\n",
        "                    relative_page = element[\"page\"]\n",
        "                    page_num = start_page + relative_page\n",
        "                    coordinates = element[\"bounding_box\"]\n",
        "                    output_page_size = page_sizes[relative_page]\n",
        "                    pdf_image = self.pdf_to_image(page_num)\n",
        "                    normalized_coordinates = self.normalize_coordinates(\n",
        "                        coordinates, output_page_size\n",
        "                    )\n",
        "\n",
        "                    # 페이지별 figure 카운트 관리\n",
        "                    if page_num not in figure_count:\n",
        "                        figure_count[page_num] = 1\n",
        "                    else:\n",
        "                        figure_count[page_num] += 1\n",
        "\n",
        "                    # 출력 파일명 생성\n",
        "                    output_file = os.path.join(\n",
        "                        output_folder,\n",
        "                        f\"page_{page_num}_figure_{figure_count[page_num]}.png\",\n",
        "                    )\n",
        "\n",
        "                    self.crop_image(pdf_image, normalized_coordinates, output_file)\n",
        "\n",
        "                    # HTML에서 이미지 경로 업데이트\n",
        "                    soup = BeautifulSoup(element[\"html\"], \"html.parser\")\n",
        "                    img_tag = soup.find(\"img\")\n",
        "                    if img_tag:\n",
        "                        # 상대 경로로 변경\n",
        "                        relative_path = os.path.relpath(output_file, output_folder)\n",
        "                        img_tag[\"src\"] = relative_path.replace(\"\\\\\", \"/\")\n",
        "                    element[\"html\"] = str(soup)\n",
        "\n",
        "                    print(f\"이미지 저장됨: {output_file}\")\n",
        "\n",
        "                html_content.append(element[\"html\"])\n",
        "\n",
        "        # HTML 파일 저장\n",
        "        html_output_file = os.path.join(output_folder, f\"{self.filename}.html\")\n",
        "\n",
        "        combined_html_content = \"\\n\".join(html_content)\n",
        "        soup = BeautifulSoup(combined_html_content, \"html.parser\")\n",
        "        all_tags = set([tag.name for tag in soup.find_all()])\n",
        "        html_tag_list = [tag for tag in list(all_tags) if tag not in [\"br\"]]\n",
        "\n",
        "        with open(html_output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(combined_html_content)\n",
        "\n",
        "        print(f\"HTML 파일이 {html_output_file}에 저장되었습니다.\")\n",
        "\n",
        "        # Markdown 파일 저장\n",
        "        md_output_file = os.path.join(output_folder, f\"{self.filename}.md\")\n",
        "\n",
        "        md_output = markdown(\n",
        "            combined_html_content,\n",
        "            convert=html_tag_list,\n",
        "        )\n",
        "\n",
        "        with open(md_output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(md_output)\n",
        "\n",
        "        print(f\"Markdown 파일이 {md_output_file}에 저장되었습니다.\")"
      ],
      "metadata": {
        "id": "BTbDLECFam1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_processor = PDFImageProcessor(SAMPLE)\n",
        "image_processor.extract_images()"
      ],
      "metadata": {
        "id": "w2hceMP5aoEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# .md 파일을 .txt 파일로 변환하는 코드\n",
        "input_md_path = \"/content/일일 착용 콘택트렌즈의 연속 착용에 따른 세균 오염/일일 착용 콘택트렌즈의 연속 착용에 따른 세균 오염.md\"  # .md 파일 경로\n",
        "output_txt_path = \"/content/일일 착용 콘택트렌즈의 연속 착용에 따른 세균 오염/일일 착용 콘택트렌즈의 연속 착용에 따른 세균 오염.txt\"  # 변환될 .txt 파일 경로\n",
        "\n",
        "# .md 파일을 읽고, .txt 파일로 저장\n",
        "with open(input_md_path, \"r\", encoding=\"utf-8\") as md_file:\n",
        "    content = md_file.read()\n",
        "\n",
        "with open(output_txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
        "    txt_file.write(content)\n",
        "\n",
        "print(\"변환이 완료되었습니다. 텍스트 파일로 저장되었습니다.\")"
      ],
      "metadata": {
        "id": "0GTBg9yvaqBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = output_txt_path\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "  raw_text = file.read()\n",
        "\n",
        "import re\n",
        "\n",
        "# 1. Markdown의 헤더, 리스트, 불필요한 기호 제거\n",
        "cleaned_text = re.sub(r'\\[.*?\\]\\(.*?\\)', '', raw_text)    # 링크 제거\n",
        "cleaned_text = re.sub(r'#+\\s?', '', cleaned_text)          # 헤더 제거\n",
        "cleaned_text = re.sub(r'\\*+', '', cleaned_text)            # 리스트 마커 제거\n",
        "cleaned_text = re.sub(r'[!#$]+', '', cleaned_text)         # 불필요한 특수문자 제거\n",
        "cleaned_text = re.sub(r'\\n{2,}', '\\n', cleaned_text)       # 연속 개행 제거\n",
        "\n",
        "# 2. 텍스트가 비어 있지 않은지 확인하고 일부만 출력하여 확인\n",
        "if cleaned_text.strip():  # 텍스트가 비어있지 않은 경우\n",
        "    print(\"전처리된 텍스트 일부:\\n\", cleaned_text[:1000])  # 일부만 출력하여 확인\n",
        "else:\n",
        "    print(\"전처리 결과에 유효한 텍스트가 없습니다.\")"
      ],
      "metadata": {
        "id": "7YlTIbg7ar18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "\n",
        "processed_text = re.sub(r'(?<![가-힣])\\b[A-Za-z]+\\b(?![가-힣])', '', cleaned_text)\n",
        "# 1. 숫자와 특수문자 뒤에 오는 이스케이프 문자 제거\n",
        "processed_text = re.sub(r'\\\\[A-Za-z0-9]+', '', cleaned_text)\n",
        "\n",
        "# 2. 논문 레퍼런스 번호 패턴 제거 (e.g., (1), (12))\n",
        "processed_text = re.sub(r'\\(\\d+\\)', '', processed_text)\n",
        "\n",
        "# 3. 기타 특수 문자 제거 (이스케이프 문자와 하이픈 위치 수정)\n",
        "processed_text = re.sub(r'[\\\\=:;@+*]', '', processed_text)  # <- 여기서 처리 완료했으므로 아래 중복 제거 필요 없음\n",
        "\n",
        "# 4. 다중 공백을 하나의 공백으로 치환\n",
        "processed_text = re.sub(r'\\s+', ' ', processed_text)\n",
        "\n",
        "# 5. 중복되는 \"===\" 형태의 구분 기호 제거\n",
        "processed_text = re.sub(r'=+', '', processed_text)\n",
        "\n",
        "processed_text = re.sub(r\"(Fig\\.|Table)\\s?\\d.*\", '', processed_text)\n",
        "\n",
        "# 8. 문장을 리스트로 분리 (마침표, 물음표, 느낌표를 기준으로)\n",
        "def split_sentences(text):\n",
        "    sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
        "    return sentences\n",
        "\n",
        "# 9. 한국어와 한국어 사이의 불필요한 영어 단어 제거\n",
        "def remove_english_between_korean(sentences):\n",
        "    cleaned_sentences = []\n",
        "    for sentence in sentences:\n",
        "        cleaned_sentence = re.sub(r'(?<=[가-힣])\\s*[A-Za-z]+\\s*(?=[가-힣])', '', sentence)\n",
        "        cleaned_sentences.append(cleaned_sentence)\n",
        "    return cleaned_sentences\n",
        "\n",
        "sentences = split_sentences(processed_text)\n",
        "processed_sentences = remove_english_between_korean(sentences)\n",
        "\n",
        "final_text = ' '.join(processed_sentences)\n",
        "\n",
        "wrapped_text = textwrap.fill(final_text, width=80)\n",
        "\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "id": "AE-72XD3at5K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}